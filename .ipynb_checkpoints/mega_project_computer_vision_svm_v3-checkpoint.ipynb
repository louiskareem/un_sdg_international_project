{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import all the important packages that will be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# provisioning\n",
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "\n",
    "# visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# prediction\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.image import img_to_array\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# metrics\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Directory and define the labels/features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir = 'organic_and_recyclable'\n",
    "features = ['O', 'R']\n",
    "data = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clean the data and feature selection. Here we will use grayscale as features for faster and scalable predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_training_data():\n",
    "    for feature in features:\n",
    "        path = os.path.join(dir, feature)\n",
    "        label =  features.index(feature)\n",
    "\n",
    "        for image in os.listdir(path):\n",
    "            image_path = os.path.join(path, image)\n",
    "            \n",
    "            # cv2.IMREAD_COLOR: It specifies to load a color image. \n",
    "            # Any transparency of image will be neglected. It is the default flag. \n",
    "            # Alternatively, we can pass integer value 1 for this flag.\n",
    "            # cv2.IMREAD_GRAYSCALE: It specifies to load an image in grayscale mode. \n",
    "            # Alternatively, we can pass integer value 0 for this flag.\n",
    "            # cv2.IMREAD_UNCHANGED: It specifies to load an image as such including alpha channel. \n",
    "            # Alternatively, we can pass integer value -1 for this flag.\n",
    "            waste_image = cv2.imread(image_path, 0)\n",
    "            \n",
    "            # resize image\n",
    "            waste_image =  cv2.resize(waste_image, (200,200))\n",
    "            \n",
    "            # flatten array to 1D\n",
    "            images = np.array(waste_image).flatten()\n",
    "\n",
    "            # convert from integers to floats\n",
    "            images = images.astype('float32')\n",
    "            \n",
    "            # normalize to the range 0-1\n",
    "            images /= 255.0\n",
    "            \n",
    "            # add images and label to data array\n",
    "            data.append([images, label])        \n",
    "        \n",
    "create_training_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's return the length of the data array. \n",
    "Below you will see that we have 671 images in total."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "671\n"
     ]
    }
   ],
   "source": [
    "print(len(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create X and y for splitting the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before splitting our data, we have to separate the values. \n",
    "So, X will hold the images and y will hold the labels (0 & 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "y = []\n",
    "\n",
    "for d, feature in data:    \n",
    "    X.append(d)\n",
    "    y.append(feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Return the unique number of values for each label. In this case 0 is Organic and 1 is Recyclable.\n",
    "Below shows that there is 315 images labeled as organic and 356 images labeled as recyclable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0 315]\n",
      " [  1 356]]\n"
     ]
    }
   ],
   "source": [
    "unique, counts = np.unique(y, return_counts=True)\n",
    "print(np.asarray((unique, counts)).T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split the data into training and tests sets. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test size here is 30% which means 70% is being used for training set.\n",
    "\n",
    "ref: https://towardsdatascience.com/3-things-you-need-to-know-before-you-train-test-split-869dfabb7e50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify= y, test_size=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit the model using Support Vector Machine in GridSearchCV. \n",
    "This will search for the best parameters and return the best score for the model.\n",
    "\n",
    "As you can see the best score our model returns is 72.27% \n",
    "and the best parameter used is Kernel RBF with degree of 1, C of 1 and gamma of 0.001\n",
    "\n",
    "ref: https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.722740791580874\n",
      "{'C': 1, 'degree': 1, 'gamma': 0.001, 'kernel': 'rbf'}\n"
     ]
    }
   ],
   "source": [
    "param_grid = { 'C':[0.1,1,100,1000],\n",
    "              'kernel':['rbf','poly','sigmoid','linear'],\n",
    "              'degree':[1,2,3,4,5,6],\n",
    "              'gamma': [1, 0.1, 0.01, 0.001, 0.0001]}\n",
    "gridSC = GridSearchCV(SVC(),param_grid)\n",
    "gridSC.fit(X_train,y_train)\n",
    "print(gridSC.best_score_)\n",
    "print(gridSC.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After getting the best score and best parameters. \n",
    "We can predict and return the accuracy score on the real labels and predicted labels, this returns 68.316% accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy in percentage: 68.31683168316832\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.47      0.58        95\n",
      "           1       0.65      0.87      0.74       107\n",
      "\n",
      "    accuracy                           0.68       202\n",
      "   macro avg       0.71      0.67      0.66       202\n",
      "weighted avg       0.70      0.68      0.67       202\n",
      "\n"
     ]
    }
   ],
   "source": [
    "grid_predictions = gridSC.predict(X_test)\n",
    "\n",
    "# return the accuracy score of prediction\n",
    "print(\"Accuracy in percentage:\", metrics.accuracy_score(y_test, grid_predictions)*100)\n",
    "print(classification_report(y_test, grid_predictions,  labels=np.unique(y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Overview of metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can run a confusion matrix to have better visualization of the results. \n",
    "Below you will see that we have 93 true positives and 45 true negatives. \n",
    "\n",
    "\"The objective of the model is to increase the values of True Positives and True Negatives \n",
    "while bringing the values of False Positives and False Negatives to zero.\"\n",
    "\n",
    "ref: https://blogs.oracle.com/ai-and-datascience/post/a-simple-guide-to-building-a-confusion-matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True negative: 45  -  False positive: 50  -  False negative: 14  -  True positive: 93\n",
      "[[45 50]\n",
      " [14 93]]\n"
     ]
    }
   ],
   "source": [
    "tn, fp, fn, tp = confusion_matrix(y_test, grid_predictions).ravel()\n",
    "print('True negative:',tn, ' - ', 'False positive:', fp, ' - ', 'False negative:', fn, ' - ', 'True positive:', tp)\n",
    "\n",
    "cm = confusion_matrix(y_test, grid_predictions)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('svm_classifier_pickle', 'wb') as file:\n",
    "    pickle.dump(gridSC, file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
